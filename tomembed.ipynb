{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset,DataLoader,TensorDataset\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,in_dim ,n_hidden_1,n_hidden_2,n_hidden_3,n_hidden_4,out_dim):  \n",
    "        super(Net,self).__init__()\n",
    "        self.w1=torch.nn.Parameter(torch.randn(n_hidden_1,in_dim, requires_grad =True))  \n",
    "        self.b1=torch.nn.Parameter(torch.zeros(n_hidden_1,requires_grad =True))\n",
    "        self.fc1=nn.Sequential(nn.ReLU(True),nn.Dropout(0.5))\n",
    "        self.fc2=nn.Sequential(nn.Linear(n_hidden_1,n_hidden_2),nn.ReLU(True),nn.Dropout(0.1))\n",
    "        self.fc3=nn.Sequential(nn.Linear(n_hidden_2,n_hidden_3),nn.ReLU(True),nn.Dropout(0.1))\n",
    "        self.fc4=nn.Sequential(nn.Linear(n_hidden_3,n_hidden_4),nn.ReLU(True),nn.Dropout(0.1))\n",
    "        self.fc5=nn.Sequential(nn.Linear(n_hidden_4,out_dim))\n",
    "    def forward(self,x,partition):\n",
    "        x=torch.add(x.matmul(self.w1.mul(partition).permute(1,0)),self.b1)\n",
    "       \n",
    "        x=self.fc1(x)                            \n",
    "        x=self.fc2(x)\n",
    "        x=self.fc3(x)\n",
    "        x=self.fc4(x)\n",
    "        out=self.fc5(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file1 = '表达矩阵.csv'\n",
    "file2 = 'TOM.csv'\n",
    "expression = np.loadtxt(file1, dtype=float, delimiter=\",\")\n",
    "partition = np.loadtxt(file2, dtype=float, delimiter=\",\")\n",
    "partition =torch.from_numpy(partition) \n",
    "partition.requires_grad=False\n",
    "labels = np.array(expression[:,-1], dtype=int)\n",
    "labels -=1\n",
    "label_vec = torch.from_numpy(labels)\n",
    "expression = torch.from_numpy(np.array(expression[:,:-1]))\n",
    "in_dim = expression.size()[1]\n",
    "n_hidden_1 = in_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preparation():\n",
    "    weight_p, bias_p = [],[]\n",
    "    for name, p in net.named_parameters():\n",
    "        if 'bias' in name:\n",
    "            bias_p += [p]\n",
    "        elif 'b1' in name:\n",
    "            bias_p += [p]\n",
    "        else:\n",
    "            weight_p += [p]\n",
    "    optimizer =torch.optim.Adam([{'params': weight_p, 'weight_decay':0.01},   \n",
    "                      {'params': bias_p, 'weight_decay':0}],lr=0.001)\n",
    "    cost = nn.CrossEntropyLoss()   \n",
    "    return optimizer,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x_train,x_test,y_train,y_test):\n",
    "    dataset1 = TensorDataset(x_train,y_train)\n",
    "    train_loader = DataLoader(dataset=dataset1,batch_size=8,shuffle=False)\n",
    "    dataset2 = TensorDataset(x_test,y_test)\n",
    "    test_loader =DataLoader(dataset=dataset2,batch_size=x_test.size()[0],shuffle=False)\n",
    "    return dataset1,dataset2,train_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_learning_rate2(optimizer, epoch, lr): \n",
    "    if epoch <=20:\n",
    "        lr =lr\n",
    "    \n",
    "    else:\n",
    "        lr = lr*(0.1 ** ((epoch-20) // 5+1))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnf(true_value, prediction_value): \n",
    "    cnf_matrix = confusion_matrix(true_value, prediction_value)  \n",
    "\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)    \n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)   \n",
    "    TP = np.diag(cnf_matrix)  \n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float) \n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "\n",
    "    ACC = accuracy_score(true_value, prediction_value)\n",
    "\n",
    "    TPR = metrics.recall_score(true_value, prediction_value, average='macro')  \n",
    "\n",
    "    TNR = np.mean(TN / (TN + FP))\n",
    "\n",
    "    F1_score = metrics.f1_score(true_value, prediction_value, average='macro')\n",
    "    return ACC,TPR,TNR,F1_score,cnf_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunningStats1: \n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = torch.zeros(n_hidden_1,in_dim)\n",
    "        self.new_m = torch.zeros_like(self.old_m)\n",
    "        self.old_s = torch.zeros_like(self.old_m)\n",
    "        self.new_s = torch.zeros_like(self.old_m)\n",
    "        self.liwai = torch.zeros_like(self.old_m)\n",
    "\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0.\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else self.liwai\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else self.liwai\n",
    "\n",
    "    def standard_deviation(self):\n",
    "      \n",
    "        return self.variance().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RunningStats2:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.old_m = torch.zeros(128,n_hidden_1)\n",
    "        self.new_m = torch.zeros_like(self.old_m)\n",
    "        self.old_s = torch.zeros_like(self.old_m)\n",
    "        self.new_s = torch.zeros_like(self.old_m)\n",
    "        self.liwai = torch.zeros_like(self.old_m)\n",
    "    def clear(self):\n",
    "        self.n = 0\n",
    "\n",
    "    def push(self, x):\n",
    "        self.n += 1\n",
    "\n",
    "        if self.n == 1:\n",
    "            self.old_m = self.new_m = x\n",
    "            self.old_s = 0.\n",
    "        else:\n",
    "            self.new_m = self.old_m + (x - self.old_m) / self.n\n",
    "            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n",
    "\n",
    "            self.old_m = self.new_m\n",
    "            self.old_s = self.new_s\n",
    "\n",
    "    def mean(self):\n",
    "        return self.new_m if self.n else self.liwai\n",
    "\n",
    "    def variance(self):\n",
    "        return self.new_s / (self.n - 1) if self.n > 1 else self.liwai\n",
    "\n",
    "    def standard_deviation(self):\n",
    "      #  return math.sqrt(self.variance())\n",
    "        return self.variance().sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):     \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed) \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "skf = StratifiedKFold(n_splits=10,random_state=0)\n",
    "fold = 0\n",
    "ACC = []\n",
    "SPE = []\n",
    "TPR = []\n",
    "F1_score = []\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "for train_index,test_index in skf.split(expression,label_vec):\n",
    "    print('fold[{}/{}]'.format(fold+1,10))\n",
    "    x_train,x_test = expression[train_index] ,expression[test_index]\n",
    "    y_train,y_test = label_vec[train_index] , label_vec[test_index]\n",
    "    dataset1,dataset2,train_loader,test_loader = preprocess(x_train,x_test,y_train,y_test)\n",
    "    net =Net( in_dim,n_hidden_1,n_hidden_2=128,n_hidden_3=32,n_hidden_4=16,out_dim=6)\n",
    "    #optimizer,cost,lr_init = preparation()\n",
    "    optimizer,cost = preparation()\n",
    "    lr_init = optimizer.param_groups[0]['lr']\n",
    "    setup_seed(120)\n",
    "    def weigth_init(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0,0.1)\n",
    "            m.bias.data.zero_()\n",
    "    net.apply(weigth_init)  #完成权重和偏置的初始化\n",
    "    num_epo =50\n",
    "    train_losses = []\n",
    "    train_acces = []\n",
    "    \n",
    "    rs_1 = RunningStats1()\n",
    "    rs_2 = RunningStats2()\n",
    "    rs_3 = RunningStats3()\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epo):\n",
    "        print('epoch[{}/{}]'.format(epoch+1,num_epo))\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        adjust_learning_rate(optimizer, epoch, lr_init)\n",
    "        terminate_w1 = list(net.parameters())[0]*1000\n",
    "        terminate_w1 = terminate_w1.detach()   \n",
    "        terminate_w2 = list(net.parameters())[2]*1000\n",
    "        terminate_w2 = terminate_w2.detach()\n",
    "        rs_1.push(terminate_w1)\n",
    "        rs_2.push(terminate_w2)\n",
    "        \n",
    "        for batch_x,batch_y in train_loader:\n",
    "            batch_x = batch_x.float()\n",
    "            partition = partition.float()\n",
    "            batch_y = batch_y.long()  \n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            out = net(batch_x,partition)\n",
    "            loss =cost(out,batch_y)   \n",
    "            train_loss += loss.item()\n",
    "            pred = torch.max(out,1)[1]\n",
    "            train_correct=(pred==batch_y).sum()\n",
    "            train_acc+=train_correct.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        train_losses.append(train_loss / len(dataset1))\n",
    "        train_acces.append(train_acc/len(dataset1))\n",
    "        print('Train Loss: {:.6f}, Acc: {:.6f}'.format(train_loss / (len(\n",
    "        dataset1)), train_acc / (len(dataset1))))\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        lastweight_1 =list(net.parameters())[0]\n",
    "        lastweight_2 =list(net.parameters())[2]\n",
    "        stdev_1 = rs_1.standard_deviation()\n",
    "        stdev_2 = rs_2.standard_deviation()\n",
    "        score_1 = torch.sum(torch.mul(stdev_1,torch.abs(torch.mul(lastweight_1,partition))),1)\n",
    "        score_1 = (score_1-torch.min(score_1))/(torch.max(score_1)-torch.min(score_1))\n",
    "        score_2 = torch.sum(torch.mul(stdev_2,torch.abs(lastweight_2)),0)\n",
    "        score = score_1+score_2\n",
    "        finalscore =(score-torch.min(score))/(torch.max(score)-torch.min(score))\n",
    "          \n",
    "    rs_3.push(finalscore)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        eval_loss = 0.\n",
    "        eval_acc = 0.\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x=batch_x.float()\n",
    "            batch_y=batch_y.long()\n",
    "            batch_x, batch_y = Variable(batch_x), Variable(batch_y)\n",
    "            out = net(batch_x,partition)\n",
    "            loss = cost(out, batch_y)\n",
    "            eval_loss += loss.item()\n",
    "            pred = torch.max(out, 1)[1]\n",
    "            num_correct = (pred == batch_y).sum()\n",
    "            eval_acc += num_correct.item()\n",
    "        batch_y = batch_y\n",
    "        ACC_val, TPR_val, SPE_val, F1_score_val, cnf_matrix = cnf(batch_y, pred)\n",
    "        ACC.append(ACC_val)\n",
    "        SPE.append(SPE_val)\n",
    "        TPR.append(TPR_val)\n",
    "        F1_score.append(F1_score_val)\n",
    "        out = out.numpy()\n",
    "        y_one_hot = label_binarize(batch_y, np.arange(6)) \n",
    "        \n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_one_hot.ravel(), out.ravel())\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0          \n",
    "        roc_auc = auc(fpr, tpr)   \n",
    "        aucs.append(roc_auc)    \n",
    "        print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss / (len(\n",
    "        dataset2)), eval_acc / (len(dataset2))))\n",
    "   \n",
    "    break\n",
    "    \n",
    "    fold +=1\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)   \n",
    "mean_tpr[-1] = 1.0  \n",
    "mean_auc = auc(mean_fpr, mean_tpr)  \n",
    "\n",
    "importance1 = np.array(rs_3.mean())\n",
    "\n",
    "outfile_1 ='xx.csv'\n",
    "\n",
    "np.savetxt(outfile_1, importance1, delimiter=\",\")\n",
    "\n",
    "print( 'ACC:', np.mean(ACC), 'TPR:', np.mean(TPR),\n",
    "          'SPE', np.mean(SPE), 'F1_score:', np.mean(F1_score),'AUC:',mean_auc)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
